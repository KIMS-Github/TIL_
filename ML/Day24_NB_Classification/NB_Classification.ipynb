{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 나이즈베이즈 분류모형(Naive Bayes classification model)에서는 모든 차원의 개별 독립변수가 서로 조건부독립(conditional independent)이라는 가정을 사용한다. 이러한 가정을 나이브 가정(naive assumption)이라고 한다.\n",
    "    GaussianNB: 정규분포 나이브베이즈\n",
    "    BernoulliNB: 베르누이분포 나이브베이즈\n",
    "    MultinomialNB: 다항분포 나이브베이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "rv0 = sp.stats.multivariate_normal([-2, -2], [[1, 0.9], [0.9, 2]])\n",
    "rv1 = sp.stats.multivariate_normal([2, 2], [[1.2, -0.8], [-0.8, 2]])\n",
    "X0 = rv0.rvs(40)\n",
    "X1 = rv1.rvs(60)\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.hstack([np.zeros(40), np.ones(60)])\n",
    "\n",
    "xx1 = np.linspace(-5, 5, 100)\n",
    "xx2 = np.linspace(-5, 5, 100)\n",
    "XX1, XX2 = np.meshgrid(xx1, xx2)\n",
    "plt.grid(False)\n",
    "plt.contour(XX1, XX2, rv0.pdf(np.dstack([XX1, XX2])), cmap=mpl.cm.cool)\n",
    "plt.contour(XX1, XX2, rv1.pdf(np.dstack([XX1, XX2])), cmap=mpl.cm.hot)\n",
    "plt.scatter(X0[:, 0], X0[:, 1], marker=\"o\", c='b', label=\"y=0\")\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker=\"s\", c='r', label=\"y=1\")\n",
    "plt.legend()\n",
    "plt.title(\"데이터의 확률분포\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 스무딩¶\n",
    "표본 데이터의 수가 적은 경우에는 베르누이 모수가 0 또는 1이라는 극단적인 모수 추정값이 나올 수도 있다. 하지만 현실적으로는 실제 모수값이 이런 극단적인 값이 나올 가능성이 적다. 따라서 베르누이 모수가 0.5인 가장 일반적인 경우를 가정하여 0이 나오는 경우와 1이 나오는 경우, 두 개의 가상 표본 데이터를 추가한다. 그러면 0이나 1과 같은 극단적인 추정값이 0.5에 가까운 다음과 값으로 변한다. 이를 라플라스 스무딩(Laplace smoothing) 또는 애드원(Add-One) 스무딩이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decision tree\n",
    "     여러 가지 규칙을 순차적으로 적용하면서 독립 변수 공간을 분할하는 분류 모형이다. 분류(classification)와 회귀 분석(regression)에 모두 사용될 수 있기 때문에 CART(Classification And Regression Tree)라고도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류규칙을 정하는 방법¶\n",
    "분류 규칙을 정하는 방법은 부모 노드와 자식 노드 간의 엔트로피를 가장 낮게 만드는 최상의 독립 변수와 기준값을 찾는 것이다. 이러한 기준을 정량화한 것이 정보획득량(information gain)이다. 기본적으로 모든 독립 변수와 모든 가능한 기준값에 대해 정보획득량을 구하여 가장 정보획득량이 큰 독립 변수와 기준값을 선택한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정보획득량¶\n",
    "정보획득량(information gain)는  𝑋 라는 조건에 의해 확률 변수  𝑌 의 엔트로피가 얼마나 감소하였는가를 나타내는 값이다. 다음처럼  𝑌 의 엔트로피에서  𝑋 에 대한  𝑌 의 조건부 엔트로피를 뺀 값으로 정의된다.\n",
    "\n",
    "𝐼𝐺[𝑌,𝑋]=𝐻[𝑌]−𝐻[𝑌|𝑋]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
